# When making a new entry, copy the commented out entry, paste it, uncomment, and fill out the fields.
# If any of the fields are confusing look at previous examples for guidance.
#- abs: null
#  authors: null
#  award: null
#  bib: null
#  img: null
#  links: {}
#  short_id: null
#  site: null
#  title: null
#  venue: null
#  video_embed: null
#  tags: null
# Current tags for research areas, see the research areas page for more details:
# deform_obj_manip, 3D_afford_obj_manip, multimodal, rl_algs, auto_driving, active_perception, self_sup_rob

- abs: "Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties."
  authors: M. Yunus Seker, Oliver Kroemer
  award: null
  bib: >
    @inproceedings{seker2024leveragingsimulationbasedmodelpreconditions,
      title={Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models}, 
      author={M. Yunus Seker and Oliver Kroemer},
      year={2024},
      journal={arXiv preprint arXiv:2403.11313},
    }
  img: ../pics/leveragingsimulationbasedmodelpreconditions.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2403.11313
  short_id: leveragingsimulationbasedmodelpreconditions
  site: https://www.ri.cmu.edu/publications/leveraging-simulation-based-model-preconditions-for-fast-action-parameter-optimization-with-multiple-models/
  title: "Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models"
  venue: "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct 2024"
  video_embed: null
  tags: null

- abs: "Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90% success rate."
  authors: Zilin Si*, Kevin Lee Zhang*, Zeynep Temel, Oliver Kroemer
  award: <award>Best paper award at 2nd Workshop on Dexterous Manipulation - Design, Perception and Control, RSS 2024</award>
  bib: >
    @inproceedings{si2024tilde,
      title={Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand},
      author={Si, Zilin and Zhang, Kevin Lee and Temel, Zeynep and Kroemer, Oliver},
      journal={arXiv preprint arXiv:2405.18804},
      year={2024}
    }
  img: ../pics/tilde.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2405.18804
  short_id: tilde2024
  site: https://sites.google.com/view/tilde-
  title: "Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand"
  venue: "Robotics, Science, and Systems (RSS), July 2024"
  video_embed: null
  tags: null

- abs: "Monitoring crop nutrients can aid farmers in optimizing fertilizer use. Many existing robots rely on vision-based phenotyping, however, which can only indirectly estimate nutrient deficiencies once crops have undergone visible color changes. We present a contact-based phenotyping robot platform that can directly insert nitrate sensors into cornstalks to proactively monitor macronutrient levels in crops. This task is challenging because inserting such sensors requires sub-centimeter precision in an environment which contains high levels of clutter, lighting variation, and occlusion. To address these challenges, we develop a robust perception-action pipeline to grasp stalks, and create a custom robot gripper which mechanically aligns the sensor before inserting it into the stalk. Through experimental validation on 48 unique stalks in a cornfield in Iowa, we demonstrate our platform's capability of detecting a stalk with 94 success"
  authors: Moonyoung Lee, Aaron Berger, Dominic Guri, Kevin Zhang, Lisa Coffey, George Kantor, Oliver Kroemer
  award: null
  bib: >
    @inproceedings{lee2024towards,
      title={Towards Autonomous Crop Monitoring: Inserting Sensors in Cluttered Environments},
      author={Lee, Moonyoung and Berger, Aaron and Guri, Dominic and Zhang, Kevin and Coffey, Lisa and Kantor, George and Kroemer, Oliver},
      journal={IEEE Robotics and Automation Letters},
      year={2024},
      publisher={IEEE}
    }
  img: ../pics/cornbot.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2311.03697
      '[video]': https://www.youtube.com/watch?v=BiV99R9snWg
  short_id: cornbot2024
  site: https://kantor-lab.github.io/cornbot/
  title: "Towards Autonomous Crop Monitoring: Inserting Sensors in Cluttered Environments"
  venue: "IEEE Robotics and Automation Letters (RA-L), June 2024"
  video_embed: <iframe width="1280" height="720" src="https://www.youtube.com/embed/Xk6A5zpNspQ" title="CMU Robotics | Towards Autonomous Crop Monitoring" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  tags: null

- abs: "There is growing interest in automating agricultural tasks that require intricate and precise interaction with specialty crops, such as trees and vines. However, developing robotic solutions for crop manipulation remains a difficult challenge due to complexities involved in modeling their deformable behavior. In this study, we present a framework for learning the deformation behavior of tree-like crops under contact interaction. Our proposed method involves encoding the state of a spring-damper modeled tree crop as a graph. This representation allows us to employ graph networks to learn both a forward model for predicting resulting deformations, and a contact policy for inferring actions to manipulate tree crops. We conduct a comprehensive set of experiments in a simulated environment and demonstrate generalizability of our method on previously unseen trees."
  authors: Chung Hee Kim, Moonyoung Lee, Oliver Kroemer, George Kantor
  award: null
  bib: >
    @inproceedings{kim2024towards,
      title={Towards robotic tree manipulation: Leveraging graph representations},
      author={Kim, Chung Hee and Lee, Moonyoung and Kroemer, Oliver and Kantor, George},
      booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
      pages={11884--11890},
      year={2024},
      organization={IEEE}
    }
  img: ../pics/treegnn.gif
  links: 
      '[arXiv]': https://arxiv.org/abs/2311.07479
      '[video]': https://www.youtube.com/watch?v=FGtdYFe0W9A
  short_id: treegnn2024
  site: https://kantor-lab.github.io/tree_gnn/
  title: "Towards Robotic Tree Manipulation: Leveraging Graph Representations"
  venue: "International Conference on Robotics and Automation (ICRA), May 2024"
  video_embed: <iframe width="1280" height="720" src="https://www.youtube.com/embed/FGtdYFe0W9A" title="Towards Robotic Tree Manipulation, Leveraging Graph Representations" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  tags: null

- abs: "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms."
  authors: Open X-Embodiment Collaboration, Jacky Liang, Kevin Zhang, Mohit Sharma, Oliver Kroemer, and 287 others
  award: <award>Best Paper at ICRA 2024</award> 
  bib: >
    @inproceedings{padalkar2023open,
      title={Open x-embodiment: Robotic learning datasets and rt-x models},
      author={Padalkar, Abhishek and Pooley, Acorn and Jain, Ajinkya and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Singh, Anikait and Brohan, Anthony and others},
      journal={https://arxiv.org/abs/2310.08864},
      year={2024}
    }
  img: ../pics/rtx.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2310.08864
  short_id: rtx2024
  site: https://robotics-transformer-x.github.io./
  title: "Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
  venue: "International Conference on Robotics and Automation (ICRA), May 2024"
  video_embed: null

- abs: "Dexterous robotic manipulation in unstructured environments can aid in everyday tasks such as cleaning and caretaking. Anthropomorphic robotic hands are highly dexterous and theoretically well-suited for working in human domains, but their complex designs and dynamics often make them difficult to control. By contrast, parallel-jaw grippers are easy to control and are used extensively in industrial applications, but they lack the dexterity for various kinds of grasps and in-hand manipulations. In this work, we present DELTAHANDS, a synergistic dexterous hand framework with Delta robots. The DELTAHANDS are soft, easy to reconfigure, simple to manufacture with low-cost off-the-shelf materials, and possess high degrees of freedom that can be easily controlled. DELTAHANDS' dexterity can be adjusted for different applications by leveraging actuation synergies, which can further reduce the control complexity, overall cost, and energy consumption. We characterize the Delta robots' kinematics accuracy, force profiles, and workspace range to assist with hand design. Finally, we evaluate the versatility of DELTAHANDS by grasping a diverse set of objects and by using teleoperation to complete three dexterous manipulation tasks: cloth folding, cap opening, and cable arrangement."
  authors: Zilin Si, Kevin Lee Zhang, Oliver Kroemer, Zeynep Temel
  award: null
  bib: >
    @article{si2024deltahands,
      title={DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots},
      author={Si, Zilin and Zhang, Kevin and Kroemer, Oliver and Temel, F Zeynep},
      journal={IEEE Robotics and Automation Letters},
      year={2024},
      publisher={IEEE}
    }
  img: ../pics/DeltaHands.png
  links: 
      '[arXiv]': https://arxiv.org/pdf/2310.05266
      '[video]': https://www.youtube.com/watch?v=rjUpAsF8DDY
  short_id: deltahand2024
  site: https://sites.google.com/view/deltahands/
  title: "DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots"
  venue: "IEEE Robotics and Automation Letters (RA-L), Feb 2024"
  video_embed: null

- abs: "For robotics systems to be used in high risk, real-world situations, they have to be quickly deployable and robust to environmental changes, under-performing hardware, and mission subtask failures. Robots are often designed to consider a single sequence of mission events, with complex algorithms lowering individual subtask failure rates under some critical con- straints. Our approach is to leverage common techniques in vision and control and encode robustness into mission structure through outcome monitoring and recovery strategies, aided by a system infrastructure that allows for quick mission deployments under tight time con- straints and no central communication. We also detail lessons in rapid field robotics devel- opment and testing. Systems were developed and evaluated through real-robot experiments at an outdoor test site in Pittsburgh, Pennsylvania, USA, as well as in the 2020 Mohamed Bin Zayed International Robotics Challenge. All competition trials were completed in fully autonomous mode without RTK-GPS. Our system led to 4th place in Challenge 2 and 7th place in the Grand Challenge, and achievements like popping five balloons (Challenge 1), successfully picking and placing a block (Challenge 2), and dispensing the most water autonomously with a UAV of all teams onto an outdoor, real fire (Challenge 3)."
  authors:  Anish Bhattacharya, Akshit Gandhi, Lukas Merkle, Rohan Tiwari, Karun Warrior, Stanley Winata, Andrew Saba, Kevin Zhang, Oliver Kroemer, and Sebastian Scherer
  award: null
  bib: >
    @article{Bhattacharya-2021-128844,
      author = {Anish Bhattacharya and Akshit Gandhi and Lukas Merkle and Rohan Tiwari and Karun Warrior and Stanley Winata and Andrew Saba and Kevin Zhang and Oliver Kroemer and Sebastian Scherer},
      title = {Mission-level Robustness with Rapidly-deployed, Autonomous Aerial Vehicles by Carnegie Mellon Team Tartan at MBZIRC 2020},
      journal = {Field Robotics},
      vol = {2},
      year = {2022},
      month = {March},
      pages = {172 - 200},
    }
  img: ../pics/mbzirc2020.png
  links: 
      '[pdf]': https://fieldrobotics.net/Field_Robotics/Volume_2_files/Vol2_07.pdf
  short_id: mbzirc2020
  site: https://www.ri.cmu.edu/publications/carnegie-mellon-team-tartan-mission-level-robustness-with-rapidly-deployed-autonomous-aerial-vehicles-in-the-mbzirc-2020/
  title: "Mission-level Robustness with Rapidly-deployed, Autonomous Aerial Vehicles by Carnegie Mellon Team Tartan at MBZIRC 2020"
  venue: "Field Robotics Special Issue: The Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020, March 2022"
  video_embed: null

- abs: "Multimodal information such as tactile, proximity and force sensing is essential for performing stable contact-rich manipulations. However, coupling multimodal information and motion control still remains a challenging topic. Rather than learning a monolithic skill policy that takes in all feedback signals at all times, skills should be divided into phases and learn to only use the sensor signals applicable to that phase. This makes learning the primitive policies for each phase easier, and allows the primitive policies to be more easily reused among different skills. However, stopping and abruptly switching between each primitive policy results in longer execution times and less robust behaviours. We therefore propose a blending approach to seamlessly combining the primitive policies into a reliable combined control policy. We evaluate both time-based and state-based blending approaches. The resulting approach was successfully evaluated in simulation and on a real robot, with an augmented finger vision sensor, on: opening a cap, turning a dial and flipping a breaker tasks. The evaluations show that the blended policies with multimodal feedback can be easily learned and reliably executed."
  authors: Tetsuya Narita, Oliver Kroemer
  award: <award>Best Manipulation Paper Finalist at ICRA 2021</award> 
  bib: >
    @article{Narita-2021-128850,
      author = {Tetsuya Narita and Oliver Kroemer},
      title = {Policy Blending and Recombination for Multimodal Contact-Rich Tasks},
      journal = {IEEE Robotics and Automation Letters},
      year = {2021},
      month = {April},
      volume = {6},
      number = {2},
      pages = {2721 - 2728},
    }
  img: ../pics/policyblending.png
  links: 
      '[pdf]': https://www.ri.cmu.edu/app/uploads/2021/08/NaritaRAL2021.pdf
  short_id: policyblending
  site: https://www.ri.cmu.edu/publications/policy-blending-and-recombination-for-multimodal-contact-rich-tasks/
  title: "Policy Blending and Recombination for Multimodal Contact-Rich Tasks"
  venue: "IEEE Robotics and Automation Letters (RA-L), April 2021"
  video_embed: null

- abs: "A key challenge in intelligent robotics is creating robots that are capable of directly in- teracting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges."
  authors: Oliver Kroemer, Scott Niekum, George Konidaris
  award: null
  bib: >
    @article{Kroemer-2021-128846,
      author = {Oliver Kroemer and Scott Niekum and George Konidaris},
      title = {A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms},
      journal = {Journal of Machine Learning Research},
      year = {2021},
      month = {January},
      volume = {22},
      number = {30},
      pages = {1 - 82},
    }
  img: ../pics/robotlearningreview.png
  links: 
      '[pdf]': https://www.ri.cmu.edu/app/uploads/2021/08/KroemerJMLR2021.pdf
  short_id: robotlearningreview
  site: https://www.ri.cmu.edu/publications/a-review-of-robot-learning-for-manipulation-challenges-representations-and-algorithms/
  title: "A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms"
  venue: "Journal of Machine Learning Research, Jan 2021"
  video_embed: null

- abs: "Tactile sensing is a key sensor modality for robots interacting with their surroundings. These sensors provide a rich and diverse set of data signals that contain detailed information collected from contacts between the robot and its environment. The data is however not limited to individual contacts and can be used to extract a wide range of information about the objects in the environment as well as the actions of the robot during the interactions. In this paper, we provide an overview of tactile information and its applications in robotics. We present a hierarchy consisting of raw, contact, object, and action levels to structure the tactile information, with higher-level information often building upon lower-level information. We discuss different types of information that can be extracted at each level of the hierarchy. The paper also includes an overview of different types of robot applications and the types of tactile information that they employ. Finally we end the article with a discussion for future tactile applications which are still beyond the current capabilities of robots."
  authors: Qiang Li, Oliver Kroemer, Zhe Su, Filipe Veiga, Mohsen Kaboli, and Helge Ritter
  award: null
  bib: >
    @article{Li-2020-128848,
      author = {Qiang Li and Oliver Kroemer and Zhe Su and Filipe Veiga and Mohsen Kaboli and Helge Ritter},
      title = {A Review of Tactile Information: Perception and Action Through Touch},
      journal = {IEEE Transactions on Robotics},
      year = {2020},
      month = {December},
      volume = {30},
      number = {6},
      pages = {1619 - 1634},
    }
  img: ../pics/tactilereview.png
  links: 
      '[pdf]': https://www.ri.cmu.edu/app/uploads/2021/08/LiTRo2020.pdf
  short_id: tactilereview
  site: https://www.ri.cmu.edu/publications/a-review-of-tactile-information-perception-and-action-through-touch/
  title: "A Review of Tactile Information: Perception and Action Through Touch"
  venue: "IEEE Transactions on Robotics, Dec 2020"
  video_embed: null


- abs: "Recent progress in soft‐matter sensors has shown improved fabrication techniques, resolution, and range. However, scaling up these sensors into an information‐rich tactile skin remains largely limited by designs that require a corresponding increase in the number of wires to support each new sensing node. To address this, a soft tactile skin that can estimate force and localize contact over a continuous 15 mm2 area with a single integrated circuit and four output wires is introduced. The skin is composed of silicone elastomer loaded with randomly distributed magnetic microparticles. Upon deformation, the magnetic particles change position and orientation with respect to an embedded magnetometer, resulting in a change in the net measured magnetic field. Two experiments are reported to calibrate and estimate both location and force of surface contact. The classification algorithms can localize pressure with an accuracy of >98% on both grid and circle pattern. Regression algorithms can localize pressure to a 3 mm2 area on average. This proof‐of‐concept sensing skin addresses the increasing need for a simple‐to‐fabricate, quick‐to‐integrate, and information‐rich tactile surface for use in robotic manipulation, soft systems, and biomonitoring."
  authors: Tess Hellebrekers, Oliver Kroemer, Carmel Majidi
  award: null
  bib: >
    @article{Hellebrekers-2019-118635,
      author = {Tess Hellebrekers and Oliver Kroemer and Carmel Majidi},
      title = {Soft Magnetic Skin for Continuous Deformation Sensing},
      journal = {Advanced Intelligent Systems},
      year = {2019},
      month = {July},
      volume = {1},
      number = {4},
    }
  img: ../pics/softmagneticskin.png
  links: 
      '[pdf]': https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.201900025
  short_id: softmagneticskin
  site: https://www.ri.cmu.edu/publications/soft-magnetic-skin-for-continuous-deformation-sensing/
  title: "Soft Magnetic Skin for Continuous Deformation Sensing"
  venue: "Advanced Intelligent Systems, July 2019"
  video_embed: null

- abs: "Granular materials produce audio-frequency mechanical vibrations in air and structures when manipulated. These vibrations correlate with both the nature of the events and the intrinsic properties of the materials producing them. We therefore propose learning to use audio-frequency vibrations from contact events to estimate the flow and amount of granular materials during scooping and pouring tasks. We evaluated multiple deep and shallow learning frameworks on a dataset of 13,750 shaking and pouring samples across five different granular materials. Our results indicate that audio is an informative sensor modality for accurately estimating flow and amounts, with a mean RMSE of 2.8 g across the five materials for pouring. We also demonstrate how the learned networks can be used to pour a desired amount of material."
  authors:  Samuel Clarke, Travers Rhodes, Christopher Atkeson, and Oliver Kroemer
  award: null
  bib: >
    @conference{Clarke-2018-110446,
      author = {Samuel Clarke and Travers Rhodes and Christopher Atkeson and Oliver Kroemer},
      title = {Learning Audio Feedback for Estimating Amount and Flow of Granular Material},
      booktitle = {Proceedings of (CoRL) Conference on Robot Learning},
      year = {2018},
      month = {October},
      pages = {529 - 550},
      publisher = {Proceedings of Machine Learning Research},
    }
  img: ../pics/learningaudio.png
  links: 
      '[pdf]': http://proceedings.mlr.press/v87/clarke18a/clarke18a.pdf
      '[video]': https://www.youtube.com/watch?v=rNrjvdOb2TA
  short_id: learningaudio
  site: https://www.ri.cmu.edu/publications/learning-audio-feedback-for-estimating-amount-and-flow-of-granular-material/
  title: "Learning Audio Feedback for Estimating Amount and Flow of Granular Material"
  venue: "Conference on Robot Learning (CoRL), Oct 2018"
  video_embed: null